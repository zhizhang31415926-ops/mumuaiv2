"""å‘é‡è®°å¿†æœåŠ¡ - åŸºäºChromaDBå®ç°é•¿æœŸè®°å¿†å’Œè¯­ä¹‰æ£€ç´¢"""
import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import httpx
from app.logger import get_logger
import os
import hashlib
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from app.config import settings as app_settings
from app.models.settings import Settings

logger = get_logger(__name__)

# é…ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
# ä¼˜å…ˆä½¿ç”¨ backend/embedding ç›®å½•ï¼ˆæ‰“åŒ…åçš„å®é™…ä½ç½®ï¼‰
import sys
from pathlib import Path

if 'SENTENCE_TRANSFORMERS_HOME' not in os.environ:
    # æ ¹æ®è¿è¡Œç¯å¢ƒç¡®å®šæ¨¡å‹ç›®å½•
    if getattr(sys, 'frozen', False):
        # PyInstaller æ‰“åŒ…å - éœ€è¦æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
        exe_dir = Path(sys.executable).parent
        
        # æ£€æŸ¥é¡ºåºï¼š
        # 1. _MEIPASS/backend/embedding (ä¸´æ—¶è§£å‹ç›®å½•)
        # 2. exeåŒçº§/_internal/backend/embedding
        # 3. exeåŒçº§/backend/embedding
        possible_paths = []
        
        if hasattr(sys, '_MEIPASS'):
            possible_paths.append(Path(sys._MEIPASS) / 'backend' / 'embedding')
        
        possible_paths.extend([
            exe_dir / '_internal' / 'backend' / 'embedding',
            exe_dir / 'backend' / 'embedding',
            exe_dir / '_internal' / 'embedding',
            exe_dir / 'embedding'
        ])
        
        model_dir = None
        for path in possible_paths:
            if path.exists():
                model_dir = path
                logger.info(f"ğŸ”§ æ‰¾åˆ°æ‰“åŒ…ç¯å¢ƒæ¨¡å‹ç›®å½•: {model_dir}")
                break
        
        if model_dir:
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(model_dir)
        else:
            # æœ€åé™çº§æ–¹æ¡ˆ
            fallback_dir = exe_dir / 'embedding'
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(fallback_dir)
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é¢„æ‰“åŒ…æ¨¡å‹ï¼Œä½¿ç”¨é™çº§ç›®å½•: {fallback_dir}")
            logger.warning(f"   æ£€æŸ¥è¿‡çš„è·¯å¾„: {[str(p) for p in possible_paths]}")
    else:
        # å¼€å‘æ¨¡å¼ï¼Œä»å½“å‰æ–‡ä»¶ä½ç½®å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        base_dir = Path(__file__).parent.parent.parent
        model_dir = base_dir / 'backend' / 'embedding'
        if model_dir.exists():
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(model_dir)
            logger.info(f"ğŸ”§ è®¾ç½®å¼€å‘ç¯å¢ƒæ¨¡å‹ç›®å½•: {model_dir}")
        else:
            # é™çº§åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ embedding
            fallback_dir = base_dir / 'embedding'
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(fallback_dir)
            logger.info(f"ğŸ”§ ä½¿ç”¨é™çº§æ¨¡å‹ç›®å½•: {fallback_dir}")


class MemoryService:
    """å‘é‡è®°å¿†ç®¡ç†æœåŠ¡ - å®ç°è¯­ä¹‰æ£€ç´¢å’Œé•¿æœŸè®°å¿†"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–ChromaDBå’ŒEmbeddingæ¨¡å‹"""
        if self._initialized:
            return
            
        try:
            # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
            chroma_dir = "data/chroma_db"
            os.makedirs(chroma_dir, exist_ok=True)
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯(ä½¿ç”¨æ–°API - PersistentClient)
            self.client = chromadb.PersistentClient(path=chroma_dir)

            # Embedding é»˜è®¤é…ç½®
            default_embedding_model = app_settings.default_embedding_model or ""
            self.local_model_name = (
                default_embedding_model
                if default_embedding_model.startswith("sentence-transformers/")
                else "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            self.default_api_model_name = (
                default_embedding_model
                if default_embedding_model and not default_embedding_model.startswith("sentence-transformers/")
                else "text-embedding-3-small"
            )

            # API æ¨¡å¼ä¸‹è·³è¿‡å¯åŠ¨æœŸæœ¬åœ°æ¨¡å‹åŠ è½½ï¼Œé¿å…é¦–æ¬¡å¯åŠ¨è¢«æ¨¡å‹ä¸‹è½½é˜»å¡
            default_mode = str(app_settings.default_embedding_mode or "local").strip().lower()
            if default_mode == "api":
                self.embedding_model = None
                self._initialized = True
                logger.info("âœ… MemoryServiceåˆå§‹åŒ–æˆåŠŸï¼ˆAPIæ¨¡å¼ï¼Œå·²è·³è¿‡æœ¬åœ°Embeddingæ¨¡å‹é¢„åŠ è½½ï¼‰")
                logger.info(f"  - ChromaDBç›®å½•: {chroma_dir}")
                logger.info(f"  - æœ¬åœ°Embeddingæ¨¡å‹(æŒ‰éœ€åŠ è½½): {self.local_model_name}")
                return
            
            # åˆå§‹åŒ–å¤šè¯­è¨€embeddingæ¨¡å‹(æ”¯æŒä¸­æ–‡)
            logger.info("ğŸ”„ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹...")
            
            # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­é…ç½®çš„æ¨¡å‹ç›®å½•
            model_cache_dir = os.environ.get('SENTENCE_TRANSFORMERS_HOME', 'embedding')
            os.makedirs(model_cache_dir, exist_ok=True)
            logger.info(f"ğŸ“‚ ä½¿ç”¨æ¨¡å‹ç¼“å­˜ç›®å½•: {os.path.abspath(model_cache_dir)}")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°ç¯å¢ƒå˜é‡å’Œè·¯å¾„
            logger.info(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            logger.info(f"ğŸ“‚ æ¨¡å‹ç¼“å­˜ç›®å½•: {os.path.abspath(model_cache_dir)}")
            logger.info(f"ğŸ”§ SENTENCE_TRANSFORMERS_HOME: {os.environ.get('SENTENCE_TRANSFORMERS_HOME', 'æœªè®¾ç½®')}")
            logger.info(f"ğŸ”§ TRANSFORMERS_OFFLINE: {os.environ.get('TRANSFORMERS_OFFLINE', 'æœªè®¾ç½®')}")
            logger.info(f"ğŸ”§ HF_HUB_OFFLINE: {os.environ.get('HF_HUB_OFFLINE', 'æœªè®¾ç½®')}")
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•å†…å®¹
            abs_cache_dir = os.path.abspath(model_cache_dir)
            logger.info(f"ğŸ“‚ æ£€æŸ¥æ¨¡å‹ç¼“å­˜ç›®å½•: {abs_cache_dir}")
            
            if os.path.exists(abs_cache_dir):
                logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•å­˜åœ¨ï¼Œæ£€æŸ¥å†…å®¹...")
                try:
                    items = os.listdir(abs_cache_dir)
                    logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•å†…å®¹ ({len(items)} é¡¹): {items}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æœŸçš„æ¨¡å‹æ–‡ä»¶å¤¹
                    expected_model_dir = os.path.join(abs_cache_dir, 'models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2')
                    logger.info(f"ğŸ” æ£€æŸ¥é¢„æœŸè·¯å¾„: {expected_model_dir}")
                    
                    if os.path.exists(expected_model_dir):
                        logger.info(f"âœ… æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•!")
                        # æ£€æŸ¥å¿«ç…§ç›®å½•
                        snapshots_dir = os.path.join(expected_model_dir, 'snapshots')
                        if os.path.exists(snapshots_dir):
                            snapshots = os.listdir(snapshots_dir)
                            logger.info(f"ğŸ“ æ¨¡å‹å¿«ç…§ ({len(snapshots)} ä¸ª): {snapshots}")
                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å¿«ç…§
                            if snapshots:
                                logger.info(f"âœ… å‘ç°æœ‰æ•ˆå¿«ç…§ï¼Œå¯ä»¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•")
                        logger.warning(f"   é¢„æœŸä½ç½®: {expected_model_dir}")
                except Exception as e:
                    logger.error(f"âŒ æ£€æŸ¥æ¨¡å‹ç›®å½•å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(f"   å †æ ˆ: {traceback.format_exc()}")
            else:
                logger.warning(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {abs_cache_dir}")
            
            try:
                logger.info("ğŸ”„ å°è¯•åŠ è½½ä¸»æ¨¡å‹: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                
                # ä½¿ç”¨ç»å¯¹è·¯å¾„æ£€æŸ¥æœ¬åœ°æ¨¡å‹
                abs_cache_dir = os.path.abspath(model_cache_dir)
                local_model_path = os.path.join(
                    abs_cache_dir,
                    'models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2'
                )
                
                logger.info(f"ğŸ” æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
                logger.info(f"ğŸ” è·¯å¾„å­˜åœ¨æ£€æŸ¥: {os.path.exists(local_model_path)}")
                
                # æ£€æŸ¥å¿«ç…§ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹
                snapshots_dir = os.path.join(local_model_path, 'snapshots')
                has_valid_model = False
                if os.path.exists(snapshots_dir):
                    try:
                        snapshots = os.listdir(snapshots_dir)
                        if snapshots:
                            logger.info(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹å¿«ç…§: {snapshots}")
                            has_valid_model = True
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ£€æŸ¥å¿«ç…§å¤±è´¥: {e}")
                
                # ä¼˜å…ˆå°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½
                if has_valid_model:
                    logger.info(f"âœ… æ£€æµ‹åˆ°å®Œæ•´æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼åŠ è½½")
                    try:
                        self.embedding_model = SentenceTransformer(
                            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                            cache_folder=abs_cache_dir,
                            device='cpu',
                            trust_remote_code=True,
                            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                        )
                        logger.info("âœ… Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ (ç¦»çº¿æ¨¡å¼)")
                    except Exception as local_err:
                        logger.warning(f"âš ï¸ ç¦»çº¿æ¨¡å¼åŠ è½½å¤±è´¥: {str(local_err)}")
                        logger.info("ğŸ”„ å°è¯•åœ¨çº¿æ¨¡å¼...")
                        raise local_err
                else:
                    logger.info("ğŸ“¥ æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨ï¼Œå°†è”ç½‘ä¸‹è½½...")
                    logger.info(f"   ä¸‹è½½åå°†ä¿å­˜åˆ°: {abs_cache_dir}")
                    self.embedding_model = SentenceTransformer(
                        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                        cache_folder=abs_cache_dir,
                        device='cpu',
                        trust_remote_code=True,
                        local_files_only=False  # å…è®¸è”ç½‘ä¸‹è½½
                    )
                    logger.info("âœ… Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ (åœ¨çº¿ä¸‹è½½)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½å¤šè¯­è¨€æ¨¡å‹: {str(e)}")
                logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {repr(e)}")
                import traceback
                logger.error(f"âŒ é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: sentence-transformers/all-MiniLM-L6-v2")
                try:
                    # é™çº§åˆ°æ›´å°çš„æ¨¡å‹ä½œä¸ºå¤‡é€‰
                    self.embedding_model = SentenceTransformer(
                        'sentence-transformers/all-MiniLM-L6-v2',
                        cache_folder=model_cache_dir,
                        device='cpu',
                        trust_remote_code=False
                    )
                    logger.info("âœ… ä½¿ç”¨å¤‡ç”¨Embeddingæ¨¡å‹ (all-MiniLM-L6-v2)")
                except Exception as e2:
                    logger.error(f"âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥: {str(e2)}")
                    logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {repr(e2)}")
                    import traceback
                    logger.error(f"âŒ é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                    logger.error("ğŸ’¡ æ¨¡å‹é¦–æ¬¡ä½¿ç”¨éœ€è¦è”ç½‘ä¸‹è½½ï¼ˆçº¦420MBï¼‰")
                    logger.error("   æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ° embedding ç›®å½•")
                    logger.error(f"ğŸ’¡ æœŸæœ›çš„æ¨¡å‹ç›®å½•ç»“æ„:")
                    logger.error(f"   {os.path.abspath(model_cache_dir)}/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/")
                    raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•Embeddingæ¨¡å‹")
            
            self._initialized = True
            logger.info("âœ… MemoryServiceåˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"  - ChromaDBç›®å½•: {chroma_dir}")
            logger.info(f"  - æœ¬åœ°Embeddingæ¨¡å‹: {self.local_model_name}")
            
        except Exception as e:
            logger.error(f"âŒ MemoryServiceåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def _ensure_local_embedding_model(self) -> None:
        """æŒ‰éœ€åŠ è½½æœ¬åœ°Embeddingæ¨¡å‹ï¼ˆä»…åœ¨ä½¿ç”¨localæ¨¡å¼æ—¶è§¦å‘ï¼‰ã€‚"""
        if getattr(self, "embedding_model", None) is not None:
            return

        model_cache_dir = os.environ.get("SENTENCE_TRANSFORMERS_HOME", "embedding")
        os.makedirs(model_cache_dir, exist_ok=True)
        logger.info("ğŸ”„ æŒ‰éœ€åŠ è½½æœ¬åœ°Embeddingæ¨¡å‹...")

        try:
            self.embedding_model = SentenceTransformer(
                self.local_model_name,
                cache_folder=os.path.abspath(model_cache_dir),
                device="cpu",
                trust_remote_code=True,
                local_files_only=False,
            )
            logger.info("âœ… æœ¬åœ°Embeddingæ¨¡å‹æŒ‰éœ€åŠ è½½æˆåŠŸ")
        except Exception as exc:
            logger.error(f"âŒ æœ¬åœ°Embeddingæ¨¡å‹æŒ‰éœ€åŠ è½½å¤±è´¥: {exc}")
            raise RuntimeError("æ— æ³•åŠ è½½æœ¬åœ°Embeddingæ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–åˆ‡æ¢ä¸ºAPIæ¨¡å¼")
    
    async def _resolve_embedding_config(
        self,
        user_id: str,
        db: Optional[AsyncSession] = None,
        override: Optional[Dict[str, Any]] = None
    ) -> Dict[str, str]:
        """è§£æç”¨æˆ·å½“å‰ Embedding é…ç½®ï¼ˆæ”¯æŒæœ¬åœ°/APIæ¨¡å¼ï¼‰ã€‚"""
        config = {
            "embedding_mode": (app_settings.default_embedding_mode or "local"),
            "embedding_provider": (app_settings.default_embedding_provider or "openai"),
            "embedding_model": (app_settings.default_embedding_model or self.local_model_name),
            "embedding_api_key": (app_settings.default_embedding_api_key or ""),
            "embedding_api_base_url": (
                app_settings.default_embedding_api_base_url
                or "https://api.openai.com/v1"
            ),
        }

        if db is not None:
            try:
                result = await db.execute(select(Settings).where(Settings.user_id == user_id))
                user_settings = result.scalar_one_or_none()
                if user_settings:
                    config.update({
                        "embedding_mode": user_settings.embedding_mode or config["embedding_mode"],
                        "embedding_provider": user_settings.embedding_provider or config["embedding_provider"],
                        "embedding_model": user_settings.embedding_model or config["embedding_model"],
                        "embedding_api_key": user_settings.embedding_api_key or config["embedding_api_key"],
                        "embedding_api_base_url": user_settings.embedding_api_base_url or config["embedding_api_base_url"],
                    })
            except Exception as exc:
                logger.warning(f"âš ï¸ è¯»å–ç”¨æˆ·Embeddingé…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {exc}")

        if override:
            for key in (
                "embedding_mode",
                "embedding_provider",
                "embedding_model",
                "embedding_api_key",
                "embedding_api_base_url",
            ):
                if override.get(key) is not None:
                    config[key] = override[key]

        mode = str(config.get("embedding_mode") or "local").strip().lower()
        if mode not in ("local", "api"):
            mode = "local"
        config["embedding_mode"] = mode

        provider = str(config.get("embedding_provider") or "openai").strip().lower()
        if provider not in ("openai", "custom"):
            provider = "openai"
        config["embedding_provider"] = provider

        if mode == "local":
            config["embedding_model"] = self.local_model_name
        else:
            model = str(config.get("embedding_model") or "").strip()
            config["embedding_model"] = model or self.default_api_model_name
            config["embedding_api_base_url"] = (
                str(config.get("embedding_api_base_url") or "").strip()
                or "https://api.openai.com/v1"
            )
            config["embedding_api_key"] = str(config.get("embedding_api_key") or "").strip()

        return config

    def _build_collection_name(self, user_id: str, project_id: str, embedding_config: Dict[str, Any]) -> str:
        """æ ¹æ® embedding é…ç½®ç”Ÿæˆ collection åç§°ã€‚

        - local æ¨¡å¼ä¿æŒå†å²å‘½åï¼Œå…¼å®¹æ—§æ•°æ®ã€‚
        - api æ¨¡å¼æŒ‰ provider+model åˆ†é›†åˆï¼Œé¿å…å‘é‡ç»´åº¦å†²çªã€‚
        """
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        project_hash = hashlib.sha256(project_id.encode()).hexdigest()[:8]
        base_name = f"u_{user_hash}_p_{project_hash}"

        if embedding_config.get("embedding_mode") != "api":
            return base_name

        namespace = f"{embedding_config.get('embedding_provider', 'openai')}:{embedding_config.get('embedding_model', '')}"
        embed_hash = hashlib.sha256(namespace.encode()).hexdigest()[:8]
        return f"{base_name}_e_{embed_hash}"

    def _list_project_collection_names(self, user_id: str, project_id: str) -> List[str]:
        """åˆ—å‡ºæŸä¸ªç”¨æˆ·é¡¹ç›®å¯¹åº”çš„æ‰€æœ‰ collectionï¼ˆå«å†å²å‘½åä¸å¤šæ¨¡å‹å‘½åï¼‰ã€‚"""
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        project_hash = hashlib.sha256(project_id.encode()).hexdigest()[:8]
        prefix = f"u_{user_hash}_p_{project_hash}"
        collection_names: List[str] = []

        try:
            for item in self.client.list_collections():
                name = item if isinstance(item, str) else getattr(item, "name", "")
                if name and name.startswith(prefix):
                    collection_names.append(name)
        except Exception as exc:
            logger.warning(f"âš ï¸ è·å–collectionåˆ—è¡¨å¤±è´¥: {exc}")

        return collection_names

    async def _embed_texts_with_api(self, texts: List[str], embedding_config: Dict[str, Any]) -> List[List[float]]:
        """é€šè¿‡ OpenAI å…¼å®¹ embeddings æ¥å£ç”Ÿæˆå‘é‡ã€‚"""
        api_key = embedding_config.get("embedding_api_key", "")
        api_base_url = embedding_config.get("embedding_api_base_url", "")
        model = embedding_config.get("embedding_model", self.default_api_model_name)

        if not api_key:
            raise RuntimeError("Embedding API æ¨¡å¼å·²å¯ç”¨ï¼Œä½†æœªé…ç½® embedding_api_key")
        if not api_base_url:
            raise RuntimeError("Embedding API æ¨¡å¼å·²å¯ç”¨ï¼Œä½†æœªé…ç½® embedding_api_base_url")

        url = f"{api_base_url.rstrip('/')}/embeddings"
        payload = {"model": model, "input": texts}
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()

        items = data.get("data", [])
        if not isinstance(items, list) or not items:
            raise RuntimeError("Embedding API è¿”å›æ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘ data å­—æ®µ")

        items = sorted(items, key=lambda x: x.get("index", 0))
        embeddings = [item.get("embedding") for item in items if item.get("embedding") is not None]
        if len(embeddings) != len(texts):
            raise RuntimeError(
                f"Embedding æ•°é‡ä¸åŒ¹é…: expected={len(texts)}, actual={len(embeddings)}"
            )
        return embeddings

    async def _embed_texts(
        self,
        texts: List[str],
        embedding_config: Dict[str, Any]
    ) -> List[List[float]]:
        """æ ¹æ®é…ç½®é€‰æ‹©æœ¬åœ°/APIç”Ÿæˆå‘é‡ã€‚"""
        if embedding_config.get("embedding_mode") == "api":
            return await self._embed_texts_with_api(texts, embedding_config)

        self._ensure_local_embedding_model()
        vectors = self.embedding_model.encode(texts).tolist()
        if vectors and isinstance(vectors[0], (int, float)):
            return [vectors]
        return vectors

    def get_collection(
        self,
        user_id: str,
        project_id: str,
        embedding_config: Optional[Dict[str, Any]] = None
    ):
        """
        è·å–æˆ–åˆ›å»ºé¡¹ç›®çš„è®°å¿†é›†åˆ
        
        æ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªé¡¹ç›®æœ‰ç‹¬ç«‹çš„ collectionï¼›API embedding æ¨¡å¼æŒ‰ provider/model åˆ†é›†åˆã€‚
        """
        if embedding_config is None:
            embedding_config = {"embedding_mode": "local", "embedding_provider": "openai", "embedding_model": self.local_model_name}

        collection_name = self._build_collection_name(user_id, project_id, embedding_config)

        try:
            return self.client.get_or_create_collection(
                name=collection_name,
                metadata={
                    "user_id": user_id,
                    "project_id": project_id,
                    "embedding_mode": embedding_config.get("embedding_mode", "local"),
                    "embedding_provider": embedding_config.get("embedding_provider", "openai"),
                    "embedding_model": embedding_config.get("embedding_model", self.local_model_name),
                    "created_at": datetime.now().isoformat()
                }
            )
        except Exception as e:
            logger.error(f"âŒ è·å–collectionå¤±è´¥: {str(e)}")
            raise
    
    async def add_memory(
        self,
        user_id: str,
        project_id: str,
        memory_id: str,
        content: str,
        memory_type: str,
        metadata: Dict[str, Any],
        db: Optional[AsyncSession] = None,
        embedding_config: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memory_id: è®°å¿†å”¯ä¸€ID
            content: è®°å¿†å†…å®¹(å°†è¢«è½¬æ¢ä¸ºå‘é‡)
            memory_type: è®°å¿†ç±»å‹
            metadata: é™„åŠ å…ƒæ•°æ®
        
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            resolved_config = await self._resolve_embedding_config(
                user_id=user_id,
                db=db,
                override=embedding_config
            )
            collection = self.get_collection(user_id, project_id, resolved_config)

            # ç”Ÿæˆæ–‡æœ¬çš„å‘é‡è¡¨ç¤º
            embedding = (await self._embed_texts([content], resolved_config))[0]
            
            # å‡†å¤‡å…ƒæ•°æ®(ChromaDBè¦æ±‚æ‰€æœ‰å€¼ä¸ºåŸºç¡€ç±»å‹)
            chroma_metadata = {
                "memory_type": memory_type,
                "chapter_id": str(metadata.get("chapter_id", "")),
                "chapter_number": int(metadata.get("chapter_number", 0)),
                "importance": float(metadata.get("importance_score", 0.5)),
                "tags": json.dumps(metadata.get("tags", []), ensure_ascii=False),
                "title": str(metadata.get("title", ""))[:200],  # é™åˆ¶é•¿åº¦
                "is_foreshadow": int(metadata.get("is_foreshadow", 0)),
                "embedding_model": str(resolved_config.get("embedding_model", ""))[:200],
                "created_at": datetime.now().isoformat()
            }
            
            # æ·»åŠ ç›¸å…³è§’è‰²ä¿¡æ¯
            if metadata.get("related_characters"):
                chroma_metadata["related_characters"] = json.dumps(
                    metadata["related_characters"], 
                    ensure_ascii=False
                )
            
            # å­˜å‚¨åˆ°å‘é‡åº“
            collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[content],
                metadatas=[chroma_metadata]
            )
            
            logger.info(f"âœ… è®°å¿†å·²æ·»åŠ : {memory_id[:8]}... (ç±»å‹:{memory_type}, é‡è¦æ€§:{chroma_metadata['importance']})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def batch_add_memories(
        self,
        user_id: str,
        project_id: str,
        memories: List[Dict[str, Any]],
        db: Optional[AsyncSession] = None,
        embedding_config: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        æ‰¹é‡æ·»åŠ è®°å¿†(æ€§èƒ½æ›´å¥½)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memories: è®°å¿†åˆ—è¡¨,æ¯ä¸ªåŒ…å«idã€contentã€typeã€metadata
        
        Returns:
            æˆåŠŸæ·»åŠ çš„æ•°é‡
        """
        if not memories:
            return 0
            
        try:
            resolved_config = await self._resolve_embedding_config(
                user_id=user_id,
                db=db,
                override=embedding_config
            )
            collection = self.get_collection(user_id, project_id, resolved_config)
            
            ids = []
            documents = []
            metadatas = []
            documents_for_embedding = []
            
            # æ‰¹é‡å‡†å¤‡æ•°æ®
            for mem in memories:
                ids.append(mem['id'])
                documents.append(mem['content'])
                documents_for_embedding.append(mem['content'])
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = mem.get('metadata', {})
                chroma_metadata = {
                    "memory_type": mem['type'],
                    "chapter_id": str(metadata.get("chapter_id", "")),
                    "chapter_number": int(metadata.get("chapter_number", 0)),
                    "importance": float(metadata.get("importance_score", 0.5)),
                    "tags": json.dumps(metadata.get("tags", []), ensure_ascii=False),
                    "title": str(metadata.get("title", ""))[:200],
                    "is_foreshadow": int(metadata.get("is_foreshadow", 0)),
                    "embedding_model": str(resolved_config.get("embedding_model", ""))[:200],
                    "created_at": datetime.now().isoformat()
                }
                metadatas.append(chroma_metadata)

            # æ‰¹é‡ç”Ÿæˆembedding
            embeddings = await self._embed_texts(documents_for_embedding, resolved_config)
            
            # æ‰¹é‡æ·»åŠ 
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            logger.info(f"âœ… æ‰¹é‡æ·»åŠ è®°å¿†æˆåŠŸ: {len(memories)}æ¡")
            return len(memories)
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ·»åŠ è®°å¿†å¤±è´¥: {str(e)}")
            return 0
    
    async def search_memories(
        self,
        user_id: str,
        project_id: str,
        query: str,
        memory_types: Optional[List[str]] = None,
        limit: int = 10,
        min_importance: float = 0.0,
        chapter_range: Optional[tuple] = None,
        db: Optional[AsyncSession] = None,
        embedding_config: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢ç›¸å…³è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            query: æŸ¥è¯¢æ–‡æœ¬(ä¼šè¢«è½¬æ¢ä¸ºå‘é‡è¿›è¡Œç›¸ä¼¼åº¦æœç´¢)
            memory_types: è¿‡æ»¤ç‰¹å®šç±»å‹çš„è®°å¿†
            limit: è¿”å›ç»“æœæ•°é‡
            min_importance: æœ€ä½é‡è¦æ€§é˜ˆå€¼
            chapter_range: ç« èŠ‚èŒƒå›´ (start, end)
        
        Returns:
            ç›¸å…³è®°å¿†åˆ—è¡¨,æŒ‰ç›¸ä¼¼åº¦æ’åº
        """
        try:
            resolved_config = await self._resolve_embedding_config(
                user_id=user_id,
                db=db,
                override=embedding_config
            )
            collection = self.get_collection(user_id, project_id, resolved_config)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = (await self._embed_texts([query], resolved_config))[0]
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶ - ChromaDBè¦æ±‚ä½¿ç”¨$andç»„åˆå¤šä¸ªæ¡ä»¶
            where_filter = None
            conditions = []
            
            if memory_types:
                conditions.append({"memory_type": {"$in": memory_types}})
            if min_importance > 0:
                conditions.append({"importance": {"$gte": min_importance}})
            if chapter_range:
                conditions.append({"chapter_number": {"$gte": chapter_range[0]}})
                conditions.append({"chapter_number": {"$lte": chapter_range[1]}})
            
            # æ ¹æ®æ¡ä»¶æ•°é‡é€‰æ‹©åˆé€‚çš„æ ¼å¼
            if len(conditions) == 0:
                where_filter = None
            elif len(conditions) == 1:
                where_filter = conditions[0]
            else:
                where_filter = {"$and": conditions}
            
            # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=limit,
                where=where_filter
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            memories = []
            if results['ids'] and results['ids'][0]:
                for i in range(len(results['ids'][0])):
                    memories.append({
                        "id": results['ids'][0][i],
                        "content": results['documents'][0][i],
                        "metadata": results['metadatas'][0][i],
                        "similarity": 1 - results['distances'][0][i] if 'distances' in results else 1.0,
                        "distance": results['distances'][0][i] if 'distances' in results else 0.0
                    })
            
            logger.info(f"ğŸ” è¯­ä¹‰æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:30]}...', æ‰¾åˆ°{len(memories)}æ¡è®°å¿†")
            return memories
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {str(e)}")
            return []
    
    async def get_recent_memories(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        recent_count: int = 3,
        min_importance: float = 0.5,
        db: Optional[AsyncSession] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘å‡ ç« çš„é‡è¦è®°å¿†(ç”¨äºä¿æŒè¿è´¯æ€§)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
            recent_count: è·å–æœ€è¿‘å‡ ç« 
            min_importance: æœ€ä½é‡è¦æ€§é˜ˆå€¼
        
        Returns:
            æœ€è¿‘ç« èŠ‚çš„è®°å¿†åˆ—è¡¨,æŒ‰é‡è¦æ€§æ’åº
        """
        try:
            resolved_config = await self._resolve_embedding_config(user_id=user_id, db=db)
            collection = self.get_collection(user_id, project_id, resolved_config)
            
            # è®¡ç®—ç« èŠ‚èŒƒå›´
            start_chapter = max(1, current_chapter - recent_count)
            
            # è·å–æœ€è¿‘ç« èŠ‚çš„è®°å¿†
            results = collection.get(
                where={
                    "$and": [
                        {"chapter_number": {"$gte": start_chapter}},
                        {"chapter_number": {"$lt": current_chapter}},
                        {"importance": {"$gte": min_importance}}
                    ]
                },
                limit=100  # å…ˆè·å–è¶³å¤Ÿå¤šçš„è®°å¿†
            )
            
            memories = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    memories.append({
                        "id": results['ids'][i],
                        "content": results['documents'][i],
                        "metadata": results['metadatas'][i]
                    })
            
            # æŒ‰é‡è¦æ€§å’Œç« èŠ‚å·æ’åº
            memories.sort(
                key=lambda x: (float(x['metadata'].get('importance', 0)), 
                              int(x['metadata'].get('chapter_number', 0))),
                reverse=True
            )
            
            # è¿”å›æœ€é‡è¦çš„å‰Næ¡
            top_memories = memories[:20]
            logger.info(f"ğŸ“š è·å–æœ€è¿‘è®°å¿†: ç« èŠ‚{start_chapter}-{current_chapter-1}, æ‰¾åˆ°{len(top_memories)}æ¡")
            return top_memories
            
        except Exception as e:
            logger.error(f"âŒ è·å–æœ€è¿‘è®°å¿†å¤±è´¥: {str(e)}")
            return []
    
    async def find_unresolved_foreshadows(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        db: Optional[AsyncSession] = None
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾æœªå®Œç»“çš„ä¼ç¬”
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
        
        Returns:
            æœªå®Œç»“ä¼ç¬”åˆ—è¡¨
        """
        try:
            resolved_config = await self._resolve_embedding_config(user_id=user_id, db=db)
            collection = self.get_collection(user_id, project_id, resolved_config)
            
            # æŸ¥æ‰¾ä¼ç¬”çŠ¶æ€ä¸º1(å·²åŸ‹ä¸‹ä½†æœªå›æ”¶)çš„è®°å¿†
            results = collection.get(
                where={
                    "$and": [
                        {"is_foreshadow": 1},
                        {"chapter_number": {"$lt": current_chapter}}
                    ]
                },
                limit=50
            )
            
            foreshadows = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    foreshadows.append({
                        "id": results['ids'][i],
                        "content": results['documents'][i],
                        "metadata": results['metadatas'][i]
                    })
            
            # æŒ‰é‡è¦æ€§æ’åº
            foreshadows.sort(
                key=lambda x: float(x['metadata'].get('importance', 0)),
                reverse=True
            )
            
            logger.info(f"ğŸ£ æ‰¾åˆ°æœªå®Œç»“ä¼ç¬”: {len(foreshadows)}ä¸ª")
            return foreshadows
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾ä¼ç¬”å¤±è´¥: {str(e)}")
            return []
    
    async def build_context_for_generation(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        chapter_outline: str,
        character_names: List[str] = None
    ) -> Dict[str, Any]:
        """
        ä¸ºç« èŠ‚ç”Ÿæˆæ„å»ºæ™ºèƒ½ä¸Šä¸‹æ–‡
        
        è¿™æ˜¯æ ¸å¿ƒåŠŸèƒ½: ç»“åˆå¤šç§æ£€ç´¢ç­–ç•¥,ä¸ºAIç”Ÿæˆæä¾›æœ€ç›¸å…³çš„è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
            chapter_outline: æœ¬ç« å¤§çº²
            character_names: æ¶‰åŠçš„è§’è‰²ååˆ—è¡¨
        
        Returns:
            åŒ…å«å„ç§ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å­—å…¸
        """
        logger.info(f"ğŸ§  å¼€å§‹æ„å»ºç« èŠ‚{current_chapter}çš„æ™ºèƒ½ä¸Šä¸‹æ–‡...")
        
        # 1. è·å–æœ€è¿‘ç« èŠ‚ä¸Šä¸‹æ–‡(æ—¶é—´è¿ç»­æ€§)
        recent = await self.get_recent_memories(
            user_id, project_id, current_chapter, 
            recent_count=3, min_importance=0.5
        )
        
        # 2. è¯­ä¹‰æœç´¢ç›¸å…³è®°å¿†
        relevant = await self.search_memories(
            user_id=user_id,
            project_id=project_id,
            query=chapter_outline,
            limit=10,
            min_importance=0.4
        )
        
        # 3. æŸ¥æ‰¾æœªå®Œç»“ä¼ç¬”
        foreshadows = await self.find_unresolved_foreshadows(
            user_id, project_id, current_chapter
        )
        
        # 4. å¦‚æœæœ‰æŒ‡å®šè§’è‰²,è·å–è§’è‰²ç›¸å…³è®°å¿†
        character_memories = []
        if character_names:
            character_query = " ".join(character_names) + " è§’è‰² çŠ¶æ€ å…³ç³»"
            character_memories = await self.search_memories(
                user_id=user_id,
                project_id=project_id,
                query=character_query,
                memory_types=["character_event", "plot_point"],
                limit=8
            )
        
        # 5. è·å–é‡è¦æƒ…èŠ‚ç‚¹
        # æ³¨æ„ï¼šChromaDBçš„whereæ¡ä»¶éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä¸èƒ½åŒæ—¶ä½¿ç”¨å¤šä¸ªé¡¶å±‚æ¡ä»¶
        try:
            plot_points = await self.search_memories(
                user_id=user_id,
                project_id=project_id,
                query="é‡è¦ è½¬æŠ˜ é«˜æ½® å…³é”®",
                memory_types=["plot_point", "hook"],
                limit=5,
                min_importance=0.7
            )
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šåˆ†åˆ«æŸ¥è¯¢
            plot_points = []
            try:
                plot_points = await self.search_memories(
                    user_id=user_id,
                    project_id=project_id,
                    query="é‡è¦ è½¬æŠ˜ é«˜æ½® å…³é”®",
                    memory_types=["plot_point", "hook"],
                    limit=5
                )
            except Exception as e2:
                logger.warning(f"âš ï¸ é™çº§æŸ¥è¯¢ä¹Ÿå¤±è´¥: {str(e2)}")
                plot_points = []
        
        context = {
            "recent_context": self._format_memories(recent, "æœ€è¿‘ç« èŠ‚è®°å¿†"),
            "relevant_memories": self._format_memories(relevant, "è¯­ä¹‰ç›¸å…³è®°å¿†"),
            "character_states": self._format_memories(character_memories, "è§’è‰²ç›¸å…³è®°å¿†"),
            "foreshadows": self._format_memories(foreshadows[:5], "æœªå®Œç»“ä¼ç¬”"),
            "plot_points": self._format_memories(plot_points, "é‡è¦æƒ…èŠ‚ç‚¹"),
            "stats": {
                "recent_count": len(recent),
                "relevant_count": len(relevant),
                "character_count": len(character_memories),
                "foreshadow_count": len(foreshadows),
                "plot_point_count": len(plot_points)
            }
        }
        
        logger.info(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: æœ€è¿‘{len(recent)}æ¡, ç›¸å…³{len(relevant)}æ¡, ä¼ç¬”{len(foreshadows)}ä¸ª")
        return context
    def _format_memories(self, memories: List[Dict], section_title: str = "è®°å¿†") -> str:
        """
        æ ¼å¼åŒ–è®°å¿†åˆ—è¡¨ä¸ºæ–‡æœ¬
        
        Args:
            memories: è®°å¿†åˆ—è¡¨
            section_title: ç« èŠ‚æ ‡é¢˜
        
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not memories:
            return f"ã€{section_title}ã€‘\næš‚æ— ç›¸å…³è®°å¿†\n"
        
        lines = [f"ã€{section_title}ã€‘"]
        for i, mem in enumerate(memories, 1):
            meta = mem.get('metadata', {})
            chapter_num = meta.get('chapter_number', '?')
            mem_type = meta.get('memory_type', 'æœªçŸ¥')
            importance = float(meta.get('importance', 0.5))
            title = meta.get('title', '')
            content = mem['content']
            
            # æ ¼å¼: [åºå·] ç¬¬Xç« -ç±»å‹(é‡è¦æ€§) æ ‡é¢˜: å†…å®¹
            line = f"{i}. [ç¬¬{chapter_num}ç« -{mem_type}â˜…{importance:.1f}]"
            if title:
                line += f" {title}: {content[:100]}"
            else:
                line += f" {content[:150]}"
            lines.append(line)
        
        return "\n".join(lines) + "\n"
    
    async def delete_chapter_memories(
        self,
        user_id: str,
        project_id: str,
        chapter_id: str
    ) -> bool:
        """
        åˆ é™¤æŒ‡å®šç« èŠ‚çš„æ‰€æœ‰è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            chapter_id: ç« èŠ‚ID
        
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            collection_names = self._list_project_collection_names(user_id, project_id)
            if not collection_names:
                logger.info(f"â„¹ï¸ é¡¹ç›®{project_id[:8]}æœªæ‰¾åˆ°ä»»ä½•å‘é‡collection")
                return True

            deleted_count = 0
            for name in collection_names:
                collection = self.client.get_collection(name=name)
                results = collection.get(where={"chapter_id": chapter_id})
                if results.get('ids'):
                    collection.delete(ids=results['ids'])
                    deleted_count += len(results['ids'])

            if deleted_count > 0:
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ç« èŠ‚{chapter_id[:8]}çš„{deleted_count}æ¡è®°å¿†")
            else:
                logger.info(f"â„¹ï¸ ç« èŠ‚{chapter_id[:8]}æ²¡æœ‰è®°å¿†éœ€è¦åˆ é™¤")
            return True
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç« èŠ‚è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def delete_project_memories(
        self,
        user_id: str,
        project_id: str
    ) -> bool:
        """
        åˆ é™¤æŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰è®°å¿†(åŒ…æ‹¬å‘é‡æ•°æ®åº“)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            collection_names = self._list_project_collection_names(user_id, project_id)
            if not collection_names:
                logger.info(f"â„¹ï¸ é¡¹ç›®{project_id[:8]}æ²¡æœ‰å¯åˆ é™¤çš„å‘é‡collection")
                return True

            for name in collection_names:
                self.client.delete_collection(name=name)
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤é¡¹ç›®{project_id[:8]}çš„å‘é‡æ•°æ®åº“collection: {name}")
            return True
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é¡¹ç›®è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def update_memory(
        self,
        user_id: str,
        project_id: str,
        memory_id: str,
        content: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        db: Optional[AsyncSession] = None,
        embedding_config: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        æ›´æ–°è®°å¿†å†…å®¹æˆ–å…ƒæ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memory_id: è®°å¿†ID
            content: æ–°å†…å®¹(å¯é€‰)
            metadata: æ–°å…ƒæ•°æ®(å¯é€‰)
        
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            resolved_config = await self._resolve_embedding_config(
                user_id=user_id,
                db=db,
                override=embedding_config
            )
            collection = self.get_collection(user_id, project_id, resolved_config)
            
            update_data = {}
            
            if content:
                # é‡æ–°ç”Ÿæˆembedding
                embedding = (await self._embed_texts([content], resolved_config))[0]
                update_data['embeddings'] = [embedding]
                update_data['documents'] = [content]
            
            if metadata:
                # å‡†å¤‡æ–°çš„å…ƒæ•°æ®
                chroma_metadata = {}
                for key, value in metadata.items():
                    if isinstance(value, (list, dict)):
                        chroma_metadata[key] = json.dumps(value, ensure_ascii=False)
                    else:
                        chroma_metadata[key] = value
                update_data['metadatas'] = [chroma_metadata]
            
            if update_data:
                collection.update(
                    ids=[memory_id],
                    **update_data
                )
                logger.info(f"âœ… è®°å¿†å·²æ›´æ–°: {memory_id[:8]}...")
                return True
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æä¾›æ›´æ–°å†…å®¹")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def get_memory_stats(
        self,
        user_id: str,
        project_id: str,
        db: Optional[AsyncSession] = None
    ) -> Dict[str, Any]:
        """
        è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            resolved_config = await self._resolve_embedding_config(user_id=user_id, db=db)
            collection = self.get_collection(user_id, project_id, resolved_config)
            
            # è·å–æ‰€æœ‰è®°å¿†
            all_memories = collection.get()
            
            if not all_memories['ids']:
                return {
                    "total_count": 0,
                    "by_type": {},
                    "by_chapter": {},
                    "foreshadow_count": 0
                }
            
            # ç»Ÿè®¡å„ç±»å‹æ•°é‡
            type_counts = {}
            chapter_counts = {}
            foreshadow_count = 0
            
            for i, meta in enumerate(all_memories['metadatas']):
                mem_type = meta.get('memory_type', 'unknown')
                chapter_num = meta.get('chapter_number', 0)
                is_foreshadow = meta.get('is_foreshadow', 0)
                
                type_counts[mem_type] = type_counts.get(mem_type, 0) + 1
                chapter_counts[str(chapter_num)] = chapter_counts.get(str(chapter_num), 0) + 1
                
                if is_foreshadow == 1:
                    foreshadow_count += 1
            
            stats = {
                "total_count": len(all_memories['ids']),
                "by_type": type_counts,
                "by_chapter": chapter_counts,
                "foreshadow_count": foreshadow_count,
                "foreshadow_resolved": sum(1 for m in all_memories['metadatas'] if m.get('is_foreshadow') == 2)
            }
            
            logger.info(f"ğŸ“Š è®°å¿†ç»Ÿè®¡: æ€»è®¡{stats['total_count']}æ¡, ä¼ç¬”{foreshadow_count}ä¸ª")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}


# åˆ›å»ºå…¨å±€å®ä¾‹
memory_service = MemoryService()
            
